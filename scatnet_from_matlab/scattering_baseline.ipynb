{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "import numpy as np\n",
    "\n",
    "import scattering\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esc_utils as U\n",
    "\n",
    "def create_dataset(npz_path, split = 4, fs = 16000,\n",
    "                   augment_factor = 9,  strong = True):\n",
    "# tfrecord_pathes = pathes for train, val tfrecords    \n",
    "    with np.load(npz_path) as dataset:\n",
    "\n",
    "        train_sounds, train_labels = [], []\n",
    "        val_sounds, val_labels = [], []\n",
    "\n",
    "        for i, fold in enumerate(dataset.files):\n",
    "            sounds = dataset[fold].item()['sounds']\n",
    "            labels = dataset[fold].item()['labels']\n",
    "            \n",
    "            \n",
    "            # we'll add to dataset only samples with length >= 40000\n",
    "            idxs = list(filter(lambda i: len(sounds[i]) >= 40000,\n",
    "                               range(len(sounds))))\n",
    "            sounds = list(np.array(sounds)[idxs])\n",
    "            labels = list(np.array(labels)[idxs])\n",
    "            \n",
    "            print('Preprocessing sounds...')\n",
    "            sounds = [U.preprocess_sound(sound) for sound in sounds]\n",
    "#             print(len(sounds), len(labels))            \n",
    "                    \n",
    "            if i  == split:\n",
    "                val_sounds.extend(sounds)\n",
    "                val_labels.extend(labels)\n",
    "            else:\n",
    "                \n",
    "                print('Augmenting data...')\n",
    "                if augment_factor:\n",
    "                    augmented_sounds, augmented_labels = [], []\n",
    "                \n",
    "                    for sound, label in zip(sounds, labels):\n",
    "                        augmented_sounds.extend([U.augment_sound(sound, strong = strong)\n",
    "                                                 for _ in range(augment_factor)])\n",
    "                        augmented_labels.extend([label]*augment_factor)\n",
    "\n",
    "                    sounds.extend(augmented_sounds)\n",
    "                    labels.extend(augmented_labels) \n",
    "                \n",
    "                train_sounds.extend(sounds)\n",
    "                train_labels.extend(labels)\n",
    "                \n",
    "#         print(len(train_sounds), len(train_labels))\n",
    "#         print(len(val_sounds), len(val_labels))\n",
    "        \n",
    "        return np.array(train_sounds), np.array(train_labels), np.array(val_sounds), np.array(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count scattering coeffisients for datasets\n",
    "\n",
    "Scattering parameters can be modified (for this purpose modify input parameters to scattering.get_scattering_coefficients() function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scattering(npz_path, path_to_scatnet):\n",
    "    # create datasets\n",
    "    train_sounds, train_labels, val_sounds, val_labels = create_dataset(npz_path,\n",
    "                                                                        split = 4,\n",
    "                                                                        fs = 16000)\n",
    "    print(train_sounds.shape, val_sounds.shape)\n",
    "    \n",
    "    # count scattering coefficients\n",
    "    S_tables = []\n",
    "    for np_arr_batch in [train_sounds, val_sounds]:\n",
    "        N = np_arr_batch.shape[1]\n",
    "        # interval of length 379ms has 370*16 time points (if sr = 16000)\n",
    "        T = 4096\n",
    "\n",
    "        S_table_batch, time_count, coeffs_count = scattering.get_scattering_coefficients(path_to_scatnet,\n",
    "                                                                                         np_arr_batch,\n",
    "                                                                                         N, T, M = 2,\n",
    "                                                                                         Q = [8,1],\n",
    "                                                                                         renorm = True,\n",
    "                                                                                         log = True)\n",
    "        S_tables.append(S_table_batch)\n",
    "        print(S_table_batch.shape, time_count, coeffs_count)        \n",
    "    return S_tables, [train_labels, val_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing sounds...\n",
      "Augmenting data...\n",
      "Preprocessing sounds...\n",
      "Augmenting data...\n",
      "Preprocessing sounds...\n",
      "Augmenting data...\n",
      "Preprocessing sounds...\n",
      "Augmenting data...\n",
      "Preprocessing sounds...\n",
      "(2870, 40000) (74, 40000)\n",
      "(2870, 354, 20) 20 [1.0, 68.0, 285.0]\n",
      "(74, 354, 20) 20 [1.0, 68.0, 285.0]\n"
     ]
    }
   ],
   "source": [
    "# Can take couple of minutes to count\n",
    "# Uncomment the lines below if you want to count scattering coefficients \n",
    "\n",
    "npz_path = \"/workspace/datasets/esc/esc10/wav16.npz\"\n",
    "path_to_scatnet = '/workspace/jgusak/Matlab_pkgs/scatnet-master/'\n",
    "S_tables, labels = get_scattering(npz_path, path_to_scatnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save counted scattering coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "esc_save_path = '/home/julia/DeepVoice_data/ESC_saved/'\n",
    "\n",
    "# Uncomment the lines below if you want to save counted coefficients\n",
    "\n",
    "# np.save(esc_save_path+'esc10_wav16_augfactor9_train_x', S_tables[0])\n",
    "# np.save(esc_save_path+'esc10_wav16_augfactor9_train_y', labels[0])\n",
    "\n",
    "# np.save(esc_save_path+'esc10_wav16_val_x', S_tables[1])\n",
    "# np.save(esc_save_path+'esc10_wav16_val_y', labels[1])\n",
    "\n",
    "# X_train = S_tables[0]\n",
    "# y_train = labels[0]\n",
    "\n",
    "# X_val = S_tables[1]\n",
    "# y_val = labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create augmented train/val datasets for ESC-10 data\n",
    "\n",
    "esc_utils.py - custom script to handle ESC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load counted scattering coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "esc_save_path = '/home/julia/DeepVoice_data/ESC_saved/'\n",
    "\n",
    "X_train = np.load(esc_save_path+'esc10_wav16_augfactor9_train_x.npy')\n",
    "y_train = np.load(esc_save_path+'esc10_wav16_augfactor9_train_y.npy')\n",
    "\n",
    "X_val = np.load(esc_save_path+'esc10_wav16_val_x.npy')\n",
    "y_val = np.load(esc_save_path+'esc10_wav16_val_y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground\n",
    "\n",
    "This section contains first attempts to create CNN architectures for environment sound classification (ESC-10) using scattering coefficients.\n",
    "\n",
    "I think CNN with scattering coefficients stucked as filters (the third architecture) is the most reasonable one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (,354, 20) - size of scattering table\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = 500\n",
    "        self.conv1 = nn.Conv2d(1, 80, (350, 3))\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(5, 1), stride=(5, 1))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(80, 80, (1, 3))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        \n",
    "        self.fc1 = nn.Linear(80*1*8, self.hid_dim)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.hid_dim, self.hid_dim)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "    \n",
    "        self.out = nn.Linear(self.hid_dim, self.out_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "#         print(self.num_flat_features(x))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        num_flat_features = 1\n",
    "        for s in x.size()[1:]:\n",
    "            num_flat_features *= s\n",
    "        return num_flat_features\n",
    "        \n",
    "def update_lr(optimizer, epoch, lr):\n",
    "    new_lr = lr*0.99\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2870, 354, 20), (2870,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250\n",
      "lr = 5.362682252071848e-06, loss = 0.022600604221224785\n",
      "Epoch 251\n",
      "lr = 5.309055429551129e-06, loss = 0.040089886635541916\n",
      "Epoch 252\n",
      "lr = 5.255964875255618e-06, loss = 0.03120892308652401\n",
      "Epoch 253\n",
      "lr = 5.203405226503062e-06, loss = 0.05042804777622223\n",
      "Epoch 254\n",
      "lr = 5.151371174238032e-06, loss = 0.05614215135574341\n",
      "Epoch 255\n",
      "lr = 5.099857462495652e-06, loss = 0.013255532830953598\n",
      "Epoch 256\n",
      "lr = 5.048858887870695e-06, loss = 0.03218790143728256\n",
      "Epoch 257\n",
      "lr = 4.998370298991988e-06, loss = 0.023557674139738083\n",
      "Epoch 258\n",
      "lr = 4.9483865960020685e-06, loss = 0.00787278264760971\n",
      "Epoch 259\n",
      "lr = 4.898902730042048e-06, loss = 0.03191589191555977\n",
      "Epoch 260\n",
      "lr = 4.849913702741627e-06, loss = 0.008003533817827702\n",
      "Epoch 261\n",
      "lr = 4.801414565714211e-06, loss = 0.028784513473510742\n",
      "Epoch 262\n",
      "lr = 4.753400420057069e-06, loss = 0.009407524950802326\n",
      "Epoch 263\n",
      "lr = 4.7058664158564985e-06, loss = 0.08944593369960785\n",
      "Epoch 264\n",
      "lr = 4.658807751697933e-06, loss = 0.019341254606842995\n",
      "Epoch 265\n",
      "lr = 4.612219674180954e-06, loss = 0.008220997639000416\n",
      "Epoch 266\n",
      "lr = 4.5660974774391445e-06, loss = 0.067878857254982\n",
      "Epoch 267\n",
      "lr = 4.520436502664753e-06, loss = 0.03519256412982941\n",
      "Epoch 268\n",
      "lr = 4.4752321376381054e-06, loss = 0.03133450075984001\n",
      "Epoch 269\n",
      "lr = 4.4304798162617245e-06, loss = 0.026177434250712395\n",
      "Epoch 270\n",
      "lr = 4.386175018099107e-06, loss = 0.03202718496322632\n",
      "Epoch 271\n",
      "lr = 4.342313267918116e-06, loss = 0.05115510895848274\n",
      "Epoch 272\n",
      "lr = 4.298890135238934e-06, loss = 0.014941113069653511\n",
      "Epoch 273\n",
      "lr = 4.255901233886545e-06, loss = 0.023665040731430054\n",
      "Epoch 274\n",
      "lr = 4.213342221547679e-06, loss = 0.04246005043387413\n",
      "Epoch 275\n",
      "lr = 4.171208799332202e-06, loss = 0.043286580592393875\n",
      "Epoch 276\n",
      "lr = 4.1294967113388806e-06, loss = 0.02236240729689598\n",
      "Epoch 277\n",
      "lr = 4.088201744225492e-06, loss = 0.0633963793516159\n",
      "Epoch 278\n",
      "lr = 4.047319726783237e-06, loss = 0.020321525633335114\n",
      "Epoch 279\n",
      "lr = 4.006846529515404e-06, loss = 0.053802490234375\n",
      "Epoch 280\n",
      "lr = 3.9667780642202505e-06, loss = 0.03551661595702171\n",
      "Epoch 281\n",
      "lr = 3.927110283578048e-06, loss = 0.00994222704321146\n",
      "Epoch 282\n",
      "lr = 3.887839180742267e-06, loss = 0.02024155482649803\n",
      "Epoch 283\n",
      "lr = 3.848960788934844e-06, loss = 0.029273666441440582\n",
      "Epoch 284\n",
      "lr = 3.810471181045496e-06, loss = 0.06103944033384323\n",
      "Epoch 285\n",
      "lr = 3.772366469235041e-06, loss = 0.04692841321229935\n",
      "Epoch 286\n",
      "lr = 3.7346428045426907e-06, loss = 0.01197071373462677\n",
      "Epoch 287\n",
      "lr = 3.6972963764972635e-06, loss = 0.010727591812610626\n",
      "Epoch 288\n",
      "lr = 3.660323412732291e-06, loss = 0.010520276613533497\n",
      "Epoch 289\n",
      "lr = 3.623720178604968e-06, loss = 0.02530033513903618\n",
      "Epoch 290\n",
      "lr = 3.5874829768189183e-06, loss = 0.034487783908843994\n",
      "Epoch 291\n",
      "lr = 3.5516081470507293e-06, loss = 0.00488348538056016\n",
      "Epoch 292\n",
      "lr = 3.516092065580222e-06, loss = 0.014910105615854263\n",
      "Epoch 293\n",
      "lr = 3.4809311449244195e-06, loss = 0.03251253813505173\n",
      "Epoch 294\n",
      "lr = 3.446121833475175e-06, loss = 0.04815838858485222\n",
      "Epoch 295\n",
      "lr = 3.4116606151404235e-06, loss = 0.021886741742491722\n",
      "Epoch 296\n",
      "lr = 3.377544008989019e-06, loss = 0.006633848883211613\n",
      "Epoch 297\n",
      "lr = 3.343768568899129e-06, loss = 0.019359394907951355\n",
      "Epoch 298\n",
      "lr = 3.3103308832101376e-06, loss = 0.017119895666837692\n",
      "Epoch 299\n",
      "lr = 3.2772275743780363e-06, loss = 0.013846532441675663\n",
      "Epoch 300\n",
      "lr = 3.244455298634256e-06, loss = 0.039005350321531296\n",
      "Epoch 301\n",
      "lr = 3.2120107456479135e-06, loss = 0.0473463200032711\n",
      "Epoch 302\n",
      "lr = 3.1798906381914345e-06, loss = 0.014953621663153172\n",
      "Epoch 303\n",
      "lr = 3.14809173180952e-06, loss = 0.015300500206649303\n",
      "Epoch 304\n",
      "lr = 3.116610814491425e-06, loss = 0.07028310745954514\n",
      "Epoch 305\n",
      "lr = 3.0854447063465105e-06, loss = 0.014374607242643833\n",
      "Epoch 306\n",
      "lr = 3.0545902592830452e-06, loss = 0.022239362820982933\n",
      "Epoch 307\n",
      "lr = 3.0240443566902147e-06, loss = 0.026627084240317345\n",
      "Epoch 308\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-6cb82fc07b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julia/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = CNN(10)\n",
    "lr = 0.001\n",
    "losses = []\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = lr)\n",
    "\n",
    "X = Variable(torch.FloatTensor(X_train[:,np.newaxis,:,:]))\n",
    "y = Variable(torch.LongTensor(y_train.astype(int)))\n",
    "\n",
    "### train model\n",
    "batch_size = 100\n",
    "\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}'.format(epoch))\n",
    "    \n",
    "    permutation = torch.randperm(X.size()[0]) \n",
    "    # type(permutation) = torch.LongTensor\n",
    "    lr = update_lr(optimizer, epoch, lr)\n",
    "    for i in range(0, X.size()[0], batch_size):\n",
    "        \n",
    "        idxs = permutation[i:i + batch_size]\n",
    "        batch_input, batch_target = X[idxs], y[idxs]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_output = net(batch_input)\n",
    "\n",
    "        batch_loss = criterion(batch_output, batch_target)\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(batch_loss.data[0])\n",
    "        \n",
    "    print('lr = {}, loss = {}'.format(lr, losses[-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### test model\n",
    "XX = Variable(torch.FloatTensor(X_val[:,np.newaxis,:,:]))\n",
    "yy = Variable(torch.LongTensor(y_val.astype(int)))\n",
    "\n",
    "outputs  = net(XX)\n",
    "_, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(predicted.numpy().ravel())\n",
    "\n",
    "accuracy_score(yy.data.numpy().ravel(), predicted.numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with 3 branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (,354, 20) \n",
    "class CNN3(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(CNN3, self).__init__()\n",
    "\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = 500\n",
    "        \n",
    "        self.nn1_conv1 = nn.Conv2d(1, 80, (1, 3))\n",
    "        \n",
    "        self.nn2_conv1 = nn.Conv2d(1, 80, (3, 3))\n",
    "        self.nn2_maxpool1 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "        self.nn2_conv2 = nn.Conv2d(80, 80, (33, 1))\n",
    "        \n",
    "        self.nn3_conv1 = nn.Conv2d(1, 80, (10, 3), stride = (5, 1))\n",
    "        self.nn3_maxpool1 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "        self.nn3_conv2 = nn.Conv2d(80, 80, (28, 1))\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(80*3, 80, (1, 3))\n",
    "        self.fc1 = nn.Linear(80*1*16, self.hid_dim)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.hid_dim, self.hid_dim)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "    \n",
    "        self.out = nn.Linear(self.hid_dim, self.out_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x1, x2, x3):\n",
    "        x1 = F.relu(self.nn1_conv1(x1))\n",
    "        \n",
    "        x2 = F.relu(self.nn2_conv1(x2))\n",
    "        x2 = self.nn2_maxpool1(x2)\n",
    "        x2 = F.relu(self.nn2_conv2(x2))\n",
    "        \n",
    "        x3 = F.relu(self.nn3_conv1(x3))\n",
    "        x3 = self.nn3_maxpool1(x3)\n",
    "        x3 = F.relu(self.nn3_conv2(x3))\n",
    "        \n",
    "        \n",
    "        # conccatenate along filter axis\n",
    "        z = torch.cat([x1, x2, x3], 1)\n",
    "        z = F.relu(self.conv1(z))\n",
    "               \n",
    "        z = z.view(-1, self.num_flat_features(z))\n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = self.dropout1(z)\n",
    "        \n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = self.dropout2(z)\n",
    "        \n",
    "        z = self.out(z)\n",
    "        \n",
    "        return z\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        num_flat_features = 1\n",
    "        for s in x.size()[1:]:\n",
    "            num_flat_features *= s\n",
    "        return num_flat_features\n",
    "        \n",
    "def update_lr(optimizer, epoch, lr):\n",
    "    new_lr = lr*0.99\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2870, 1, 285, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,np.newaxis,69:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "lr = 0.00099, loss = 2.304621696472168\n",
      "Epoch 1\n",
      "lr = 0.0009801, loss = 2.1156399250030518\n",
      "Epoch 2\n",
      "lr = 0.000970299, loss = 1.8402665853500366\n",
      "Epoch 3\n",
      "lr = 0.0009605960099999999, loss = 1.8408502340316772\n",
      "Epoch 4\n",
      "lr = 0.0009509900498999999, loss = 1.6076338291168213\n",
      "Epoch 5\n",
      "lr = 0.0009414801494009999, loss = 1.5357186794281006\n",
      "Epoch 6\n",
      "lr = 0.0009320653479069899, loss = 1.3789286613464355\n",
      "Epoch 7\n",
      "lr = 0.00092274469442792, loss = 1.3700292110443115\n",
      "Epoch 8\n",
      "lr = 0.0009135172474836408, loss = 1.4399534463882446\n",
      "Epoch 9\n",
      "lr = 0.0009043820750088043, loss = 1.2949284315109253\n",
      "Epoch 10\n",
      "lr = 0.0008953382542587163, loss = 1.134507179260254\n",
      "Epoch 11\n",
      "lr = 0.0008863848717161291, loss = 1.1872352361679077\n",
      "Epoch 12\n",
      "lr = 0.0008775210229989678, loss = 0.9380912780761719\n",
      "Epoch 13\n",
      "lr = 0.0008687458127689781, loss = 1.212938904762268\n",
      "Epoch 14\n",
      "lr = 0.0008600583546412883, loss = 1.0702142715454102\n",
      "Epoch 15\n",
      "lr = 0.0008514577710948754, loss = 1.0019346475601196\n",
      "Epoch 16\n",
      "lr = 0.0008429431933839266, loss = 0.8686752319335938\n",
      "Epoch 17\n",
      "lr = 0.0008345137614500873, loss = 0.9270740151405334\n",
      "Epoch 18\n",
      "lr = 0.0008261686238355864, loss = 1.0738130807876587\n",
      "Epoch 19\n",
      "lr = 0.0008179069375972306, loss = 0.8902330994606018\n",
      "Epoch 20\n",
      "lr = 0.0008097278682212583, loss = 1.005012035369873\n",
      "Epoch 21\n",
      "lr = 0.0008016305895390457, loss = 0.9780240654945374\n",
      "Epoch 22\n",
      "lr = 0.0007936142836436553, loss = 0.8865760564804077\n",
      "Epoch 23\n",
      "lr = 0.0007856781408072188, loss = 1.0192052125930786\n",
      "Epoch 24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-94485a47144d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/julia/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    145\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = CNN3(10)\n",
    "lr = 0.001\n",
    "losses = []\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = lr)\n",
    "\n",
    "X1 = Variable(torch.FloatTensor(X_train[:,np.newaxis,:1,:]))\n",
    "X2 = Variable(torch.FloatTensor(X_train[:,np.newaxis,1:69,:]))\n",
    "X3 = Variable(torch.FloatTensor(X_train[:,np.newaxis,69:,:]))\n",
    "\n",
    "y = Variable(torch.LongTensor(y_train.astype(int)))\n",
    "\n",
    "### train model\n",
    "batch_size = 200\n",
    "\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}'.format(epoch))\n",
    "    \n",
    "    permutation = torch.randperm(X1.size()[0]) \n",
    "    # type(permutation) = torch.LongTensor\n",
    "    lr = update_lr(optimizer, epoch, lr)\n",
    "    for i in range(0, X1.size()[0], batch_size):\n",
    "        \n",
    "        idxs = permutation[i:i + batch_size]\n",
    "        batch_input1, batch_input2, batch_input3, batch_target = X1[idxs], X2[idxs], X3[idxs], y[idxs]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_output = net(batch_input1, batch_input2, batch_input3)\n",
    "\n",
    "        batch_loss = criterion(batch_output, batch_target)\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(batch_loss.data[0])\n",
    "        \n",
    "    print('lr = {}, loss = {}'.format(lr, losses[-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### test model\n",
    "XX = Variable(torch.FloatTensor(X_val[:,np.newaxis,:,:]))\n",
    "yy = Variable(torch.LongTensor(y_val.astype(int)))\n",
    "\n",
    "outputs  = net(XX)\n",
    "_, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(predicted.numpy().ravel())\n",
    "\n",
    "accuracy_score(yy.data.numpy().ravel(), predicted.numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNN for scattering stucked as filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (,354, 20) \n",
    "class CNN1(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super(CNN1, self).__init__()\n",
    "\n",
    "        self.out_dim = out_dim\n",
    "        self.hid_dim = 500\n",
    "              \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(354, 80, (1, 1))\n",
    "        self.conv2 = nn.Conv2d(80, 80, (1, 3))\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.conv3 = nn.Conv2d(80, 80, (1, 3))\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(80*1*7, self.hid_dim)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(self.hid_dim, self.hid_dim)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "    \n",
    "        self.out = nn.Linear(self.hid_dim, self.out_dim)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # conccatenate along filter axis\n",
    "        z = F.relu(self.conv1(x))\n",
    "        z = F.relu(self.conv2(z))\n",
    "        z = self.maxpool1(z)\n",
    "        z = F.relu(self.conv3(z))\n",
    "        \n",
    "               \n",
    "        z = z.view(-1, self.num_flat_features(z))\n",
    "        z = F.relu(self.fc1(z))\n",
    "        z = self.dropout1(z)\n",
    "        \n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = self.dropout2(z)\n",
    "        \n",
    "        z = self.out(z)\n",
    "        \n",
    "        return z\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        num_flat_features = 1\n",
    "        for s in x.size()[1:]:\n",
    "            num_flat_features *= s\n",
    "        return num_flat_features\n",
    "        \n",
    "def update_lr(optimizer, epoch, lr):\n",
    "    new_lr = lr*0.99\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2870, 354, 20)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870\n",
      "Epoch 502\n",
      "lr = 6.248463996372976e-06, loss = 0.025746440514922142\n",
      "Epoch 503\n",
      "lr = 6.185979356409246e-06, loss = 0.00035181784187443554\n",
      "Epoch 504\n",
      "lr = 6.124119562845154e-06, loss = 0.0024826310109347105\n",
      "Epoch 505\n",
      "lr = 6.062878367216702e-06, loss = 0.02394513227045536\n",
      "Epoch 506\n",
      "lr = 6.002249583544535e-06, loss = 0.0002961443969979882\n",
      "Epoch 507\n",
      "lr = 5.9422270877090895e-06, loss = 0.0001633830543141812\n",
      "Epoch 508\n",
      "lr = 5.8828048168319985e-06, loss = 0.00020924008276779205\n",
      "Epoch 509\n",
      "lr = 5.823976768663678e-06, loss = 0.00764023931697011\n",
      "Epoch 510\n",
      "lr = 5.765737000977042e-06, loss = 0.0008826454286463559\n",
      "Epoch 511\n",
      "lr = 5.7080796309672714e-06, loss = 0.0002618200669530779\n",
      "Epoch 512\n",
      "lr = 5.650998834657599e-06, loss = 0.0014220832381397486\n",
      "Epoch 513\n",
      "lr = 5.594488846311023e-06, loss = 0.0002802496892400086\n",
      "Epoch 514\n",
      "lr = 5.538543957847913e-06, loss = 0.00018453392840456218\n",
      "Epoch 515\n",
      "lr = 5.4831585182694335e-06, loss = 0.0004973319009877741\n",
      "Epoch 516\n",
      "lr = 5.428326933086739e-06, loss = 0.000766113109420985\n",
      "Epoch 517\n",
      "lr = 5.374043663755871e-06, loss = 0.006544385105371475\n",
      "Epoch 518\n",
      "lr = 5.320303227118312e-06, loss = 0.00038345440407283604\n",
      "Epoch 519\n",
      "lr = 5.267100194847129e-06, loss = 0.0007923924131318927\n",
      "Epoch 520\n",
      "lr = 5.2144291928986575e-06, loss = 0.00018787845328915864\n",
      "Epoch 521\n",
      "lr = 5.162284900969671e-06, loss = 0.007967587560415268\n",
      "Epoch 522\n",
      "lr = 5.110662051959974e-06, loss = 9.592805145075545e-05\n",
      "Epoch 523\n",
      "lr = 5.0595554314403746e-06, loss = 0.0011191574158146977\n",
      "Epoch 524\n",
      "lr = 5.008959877125971e-06, loss = 0.0005398998619057238\n",
      "Epoch 525\n",
      "lr = 4.958870278354711e-06, loss = 0.00018685789837036282\n",
      "Epoch 526\n",
      "lr = 4.909281575571164e-06, loss = 0.006463801022619009\n",
      "Epoch 527\n",
      "lr = 4.860188759815452e-06, loss = 0.003995848819613457\n",
      "Epoch 528\n",
      "lr = 4.8115868722172975e-06, loss = 0.0018787735607475042\n",
      "Epoch 529\n",
      "lr = 4.763471003495125e-06, loss = 0.000807036878541112\n",
      "Epoch 530\n",
      "lr = 4.715836293460174e-06, loss = 0.0008127156761474907\n",
      "Epoch 531\n",
      "lr = 4.668677930525572e-06, loss = 9.793169010663405e-05\n",
      "Epoch 532\n",
      "lr = 4.621991151220316e-06, loss = 0.00032922590617090464\n",
      "Epoch 533\n",
      "lr = 4.5757712397081135e-06, loss = 0.005659841932356358\n",
      "Epoch 534\n",
      "lr = 4.5300135273110325e-06, loss = 0.02599153481423855\n",
      "Epoch 535\n",
      "lr = 4.484713392037922e-06, loss = 0.01242442149668932\n",
      "Epoch 536\n",
      "lr = 4.4398662581175434e-06, loss = 0.0004665433370973915\n",
      "Epoch 537\n",
      "lr = 4.395467595536368e-06, loss = 0.010268366895616055\n",
      "Epoch 538\n",
      "lr = 4.351512919581005e-06, loss = 0.0013665480073541403\n",
      "Epoch 539\n",
      "lr = 4.307997790385195e-06, loss = 0.005877426825463772\n",
      "Epoch 540\n",
      "lr = 4.264917812481343e-06, loss = 0.00011705897486535832\n",
      "Epoch 541\n",
      "lr = 4.2222686343565294e-06, loss = 0.0008094177464954555\n",
      "Epoch 542\n",
      "lr = 4.180045948012964e-06, loss = 0.0015525942435488105\n",
      "Epoch 543\n",
      "lr = 4.138245488532834e-06, loss = 0.0011753382859751582\n",
      "Epoch 544\n",
      "lr = 4.096863033647506e-06, loss = 0.00014178743003867567\n",
      "Epoch 545\n",
      "lr = 4.05589440331103e-06, loss = 0.0011152316583320498\n",
      "Epoch 546\n",
      "lr = 4.0153354592779195e-06, loss = 0.0005013161571696401\n",
      "Epoch 547\n",
      "lr = 3.97518210468514e-06, loss = 4.918771082884632e-05\n",
      "Epoch 548\n",
      "lr = 3.935430283638289e-06, loss = 0.0033702426590025425\n",
      "Epoch 549\n",
      "lr = 3.896075980801906e-06, loss = 0.004492122679948807\n",
      "Epoch 550\n",
      "lr = 3.857115220993887e-06, loss = 0.00022383320902008563\n",
      "Epoch 551\n",
      "lr = 3.818544068783948e-06, loss = 0.006677555851638317\n",
      "Epoch 552\n",
      "lr = 3.7803586280961083e-06, loss = 9.414378291694447e-05\n",
      "Epoch 553\n",
      "lr = 3.742555041815147e-06, loss = 0.00023202582087833434\n",
      "Epoch 554\n",
      "lr = 3.7051294913969957e-06, loss = 0.0002951330679934472\n",
      "Epoch 555\n",
      "lr = 3.668078196483026e-06, loss = 5.8374560467200354e-05\n",
      "Epoch 556\n",
      "lr = 3.6313974145181955e-06, loss = 0.004220586270093918\n",
      "Epoch 557\n",
      "lr = 3.5950834403730136e-06, loss = 0.00015700362564530224\n",
      "Epoch 558\n",
      "lr = 3.5591326059692835e-06, loss = 0.00019425370555836707\n",
      "Epoch 559\n",
      "lr = 3.5235412799095907e-06, loss = 0.007713985629379749\n",
      "Epoch 560\n",
      "lr = 3.488305867110495e-06, loss = 0.013815020211040974\n",
      "Epoch 561\n",
      "lr = 3.45342280843939e-06, loss = 0.001759704202413559\n",
      "Epoch 562\n",
      "lr = 3.418888580354996e-06, loss = 0.0008028589654713869\n",
      "Epoch 563\n",
      "lr = 3.384699694551446e-06, loss = 0.0001293106033699587\n",
      "Epoch 564\n",
      "lr = 3.3508526976059316e-06, loss = 0.004446473438292742\n",
      "Epoch 565\n",
      "lr = 3.3173441706298722e-06, loss = 0.0016932229045778513\n",
      "Epoch 566\n",
      "lr = 3.2841707289235733e-06, loss = 0.00028038991149514914\n",
      "Epoch 567\n",
      "lr = 3.2513290216343377e-06, loss = 0.00069219502620399\n",
      "Epoch 568\n",
      "lr = 3.2188157314179942e-06, loss = 0.0003213407180737704\n",
      "Epoch 569\n",
      "lr = 3.1866275741038145e-06, loss = 0.0005668572266586125\n",
      "Epoch 570\n",
      "lr = 3.1547612983627764e-06, loss = 0.003867709543555975\n",
      "Epoch 571\n",
      "lr = 3.1232136853791485e-06, loss = 0.0011711826082319021\n",
      "Epoch 572\n",
      "lr = 3.091981548525357e-06, loss = 0.014068474061787128\n",
      "Epoch 573\n",
      "lr = 3.0610617330401034e-06, loss = 0.0017731024418026209\n",
      "Epoch 574\n",
      "lr = 3.0304511157097025e-06, loss = 0.0008238486479967833\n",
      "Epoch 575\n",
      "lr = 3.0001466045526055e-06, loss = 0.0007175751961767673\n",
      "Epoch 576\n",
      "lr = 2.9701451385070794e-06, loss = 0.000731355685275048\n",
      "Epoch 577\n",
      "lr = 2.9404436871220088e-06, loss = 0.00022843788610771298\n",
      "Epoch 578\n",
      "lr = 2.9110392502507887e-06, loss = 0.00047386554069817066\n",
      "Epoch 579\n",
      "lr = 2.8819288577482806e-06, loss = 0.0011829674476757646\n",
      "Epoch 580\n",
      "lr = 2.853109569170798e-06, loss = 0.0010495824972167611\n",
      "Epoch 581\n",
      "lr = 2.82457847347909e-06, loss = 0.0004661751736421138\n",
      "Epoch 582\n",
      "lr = 2.796332688744299e-06, loss = 0.0007387705845758319\n",
      "Epoch 583\n",
      "lr = 2.768369361856856e-06, loss = 0.009777160361409187\n",
      "Epoch 584\n",
      "lr = 2.7406856682382877e-06, loss = 0.0001317556743742898\n",
      "Epoch 585\n",
      "lr = 2.713278811555905e-06, loss = 0.014037844724953175\n",
      "Epoch 586\n",
      "lr = 2.686146023440346e-06, loss = 0.010261181741952896\n",
      "Epoch 587\n",
      "lr = 2.6592845632059426e-06, loss = 0.002733090193942189\n",
      "Epoch 588\n",
      "lr = 2.632691717573883e-06, loss = 0.00019574676116462797\n",
      "Epoch 589\n",
      "lr = 2.6063648003981444e-06, loss = 0.00019388248620089144\n",
      "Epoch 590\n",
      "lr = 2.580301152394163e-06, loss = 5.6362958275713027e-05\n",
      "Epoch 591\n",
      "lr = 2.554498140870221e-06, loss = 9.128011151915416e-05\n",
      "Epoch 592\n",
      "lr = 2.528953159461519e-06, loss = 0.013781575486063957\n",
      "Epoch 593\n",
      "lr = 2.5036636278669036e-06, loss = 0.0009225088288076222\n",
      "Epoch 594\n",
      "lr = 2.4786269915882348e-06, loss = 0.0003088284283876419\n",
      "Epoch 595\n",
      "lr = 2.4538407216723522e-06, loss = 0.0023008016869425774\n",
      "Epoch 596\n",
      "lr = 2.4293023144556287e-06, loss = 0.0004525282420217991\n",
      "Epoch 597\n",
      "lr = 2.405009291311072e-06, loss = 0.0011018677614629269\n",
      "Epoch 598\n",
      "lr = 2.3809591983979617e-06, loss = 0.00027834525099024177\n",
      "Epoch 599\n",
      "lr = 2.357149606413982e-06, loss = 0.00016047463577706367\n",
      "Epoch 600\n",
      "lr = 2.333578110349842e-06, loss = 0.0002836977655533701\n",
      "Epoch 601\n",
      "lr = 2.3102423292463435e-06, loss = 9.65419749263674e-05\n",
      "Epoch 602\n",
      "lr = 2.28713990595388e-06, loss = 0.0008045933209359646\n",
      "Epoch 603\n",
      "lr = 2.2642685068943414e-06, loss = 0.000992982997559011\n",
      "Epoch 604\n",
      "lr = 2.241625821825398e-06, loss = 0.0003365525626577437\n",
      "Epoch 605\n",
      "lr = 2.219209563607144e-06, loss = 0.007181312423199415\n",
      "Epoch 606\n",
      "lr = 2.1970174679710723e-06, loss = 3.934677079087123e-05\n",
      "Epoch 607\n",
      "lr = 2.1750472932913614e-06, loss = 0.0003367286699358374\n",
      "Epoch 608\n",
      "lr = 2.153296820358448e-06, loss = 0.0008902641711756587\n",
      "Epoch 609\n",
      "lr = 2.1317638521548633e-06, loss = 0.001794959418475628\n",
      "Epoch 610\n",
      "lr = 2.1104462136333147e-06, loss = 0.0003719787928275764\n",
      "Epoch 611\n",
      "lr = 2.0893417514969813e-06, loss = 0.00017711294640321285\n",
      "Epoch 612\n",
      "lr = 2.0684483339820113e-06, loss = 0.024383598938584328\n",
      "Epoch 613\n",
      "lr = 2.0477638506421912e-06, loss = 0.0007789889350533485\n",
      "Epoch 614\n",
      "lr = 2.0272862121357692e-06, loss = 6.938963633729145e-05\n",
      "Epoch 615\n",
      "lr = 2.0070133500144114e-06, loss = 0.0005115080275572836\n",
      "Epoch 616\n",
      "lr = 1.986943216514267e-06, loss = 0.001189388451166451\n",
      "Epoch 617\n",
      "lr = 1.9670737843491245e-06, loss = 0.0003919459122698754\n",
      "Epoch 618\n",
      "lr = 1.9474030465056334e-06, loss = 0.0019474874716252089\n",
      "Epoch 619\n",
      "lr = 1.927929016040577e-06, loss = 0.0003169453702867031\n",
      "Epoch 620\n",
      "lr = 1.9086497258801715e-06, loss = 0.0006394654628820717\n",
      "Epoch 621\n",
      "lr = 1.8895632286213697e-06, loss = 0.0005302981589920819\n",
      "Epoch 622\n",
      "lr = 1.870667596335156e-06, loss = 0.007177076302468777\n",
      "Epoch 623\n",
      "lr = 1.8519609203718044e-06, loss = 0.0048503573052585125\n",
      "Epoch 624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 1.8334413111680864e-06, loss = 0.0004516254994086921\n",
      "Epoch 625\n",
      "lr = 1.8151068980564054e-06, loss = 0.0012931247474625707\n",
      "Epoch 626\n",
      "lr = 1.7969558290758413e-06, loss = 0.0011184217873960733\n",
      "Epoch 627\n",
      "lr = 1.7789862707850829e-06, loss = 0.0023657302372157574\n",
      "Epoch 628\n",
      "lr = 1.7611964080772321e-06, loss = 0.009167482145130634\n",
      "Epoch 629\n",
      "lr = 1.7435844439964599e-06, loss = 0.0007819075835868716\n",
      "Epoch 630\n",
      "lr = 1.7261485995564952e-06, loss = 2.6003071980085224e-05\n",
      "Epoch 631\n",
      "lr = 1.7088871135609302e-06, loss = 0.0062132105231285095\n",
      "Epoch 632\n",
      "lr = 1.6917982424253209e-06, loss = 0.00291444081813097\n",
      "Epoch 633\n",
      "lr = 1.6748802600010677e-06, loss = 0.004247509874403477\n",
      "Epoch 634\n",
      "lr = 1.6581314574010571e-06, loss = 0.0057455999776721\n",
      "Epoch 635\n",
      "lr = 1.6415501428270464e-06, loss = 0.0043111094273626804\n",
      "Epoch 636\n",
      "lr = 1.625134641398776e-06, loss = 0.0038849080447107553\n",
      "Epoch 637\n",
      "lr = 1.6088832949847882e-06, loss = 0.001250638859346509\n",
      "Epoch 638\n",
      "lr = 1.5927944620349404e-06, loss = 0.0005787106929346919\n",
      "Epoch 639\n",
      "lr = 1.576866517414591e-06, loss = 0.0007121023372747004\n",
      "Epoch 640\n",
      "lr = 1.561097852240445e-06, loss = 0.0028719024267047644\n",
      "Epoch 641\n",
      "lr = 1.5454868737180404e-06, loss = 0.001148901879787445\n",
      "Epoch 642\n",
      "lr = 1.53003200498086e-06, loss = 0.003358870279043913\n",
      "Epoch 643\n",
      "lr = 1.5147316849310512e-06, loss = 0.0023786560632288456\n",
      "Epoch 644\n",
      "lr = 1.4995843680817406e-06, loss = 0.00032967323204502463\n",
      "Epoch 645\n",
      "lr = 1.4845885244009232e-06, loss = 0.00012450412032194436\n",
      "Epoch 646\n",
      "lr = 1.469742639156914e-06, loss = 0.00015317348879761994\n",
      "Epoch 647\n",
      "lr = 1.4550452127653448e-06, loss = 0.00035871370346285403\n",
      "Epoch 648\n",
      "lr = 1.4404947606376914e-06, loss = 0.0010797996073961258\n",
      "Epoch 649\n",
      "lr = 1.4260898130313144e-06, loss = 0.006227447651326656\n",
      "Epoch 650\n",
      "lr = 1.4118289149010013e-06, loss = 0.0019781026057899\n",
      "Epoch 651\n",
      "lr = 1.3977106257519913e-06, loss = 0.0006431423244066536\n",
      "Epoch 652\n",
      "lr = 1.3837335194944713e-06, loss = 0.001236503478139639\n",
      "Epoch 653\n",
      "lr = 1.3698961842995267e-06, loss = 8.189782965928316e-05\n",
      "Epoch 654\n",
      "lr = 1.3561972224565314e-06, loss = 0.0009678590577095747\n",
      "Epoch 655\n",
      "lr = 1.3426352502319662e-06, loss = 0.0006111164693720639\n",
      "Epoch 656\n",
      "lr = 1.3292088977296466e-06, loss = 0.0018575253197923303\n",
      "Epoch 657\n",
      "lr = 1.31591680875235e-06, loss = 0.0018677117768675089\n",
      "Epoch 658\n",
      "lr = 1.3027576406648265e-06, loss = 0.0016662942944094539\n",
      "Epoch 659\n",
      "lr = 1.2897300642581783e-06, loss = 0.007350339088588953\n",
      "Epoch 660\n",
      "lr = 1.2768327636155966e-06, loss = 0.019267063587903976\n",
      "Epoch 661\n",
      "lr = 1.2640644359794407e-06, loss = 0.0020321232732385397\n",
      "Epoch 662\n",
      "lr = 1.2514237916196463e-06, loss = 0.0004567223077174276\n",
      "Epoch 663\n",
      "lr = 1.2389095537034498e-06, loss = 0.0006301510147750378\n",
      "Epoch 664\n",
      "lr = 1.2265204581664152e-06, loss = 6.699037476209924e-05\n",
      "Epoch 665\n",
      "lr = 1.214255253584751e-06, loss = 0.0038597923703491688\n",
      "Epoch 666\n",
      "lr = 1.2021127010489036e-06, loss = 0.0007398714660666883\n",
      "Epoch 667\n",
      "lr = 1.1900915740384146e-06, loss = 0.001715489779599011\n",
      "Epoch 668\n",
      "lr = 1.1781906582980304e-06, loss = 0.0022999392822384834\n",
      "Epoch 669\n",
      "lr = 1.1664087517150502e-06, loss = 0.01657986082136631\n",
      "Epoch 670\n",
      "lr = 1.1547446641978997e-06, loss = 0.007228025235235691\n",
      "Epoch 671\n",
      "lr = 1.1431972175559207e-06, loss = 0.0022341646254062653\n",
      "Epoch 672\n",
      "lr = 1.1317652453803616e-06, loss = 0.01599636673927307\n",
      "Epoch 673\n",
      "lr = 1.1204475929265579e-06, loss = 0.00914250873029232\n",
      "Epoch 674\n",
      "lr = 1.1092431169972922e-06, loss = 0.00033916314714588225\n",
      "Epoch 675\n",
      "lr = 1.0981506858273192e-06, loss = 0.00010936319449683651\n",
      "Epoch 676\n",
      "lr = 1.0871691789690459e-06, loss = 0.005924123805016279\n",
      "Epoch 677\n",
      "lr = 1.0762974871793555e-06, loss = 0.0033984880428761244\n",
      "Epoch 678\n",
      "lr = 1.065534512307562e-06, loss = 0.000479523470858112\n",
      "Epoch 679\n",
      "lr = 1.0548791671844863e-06, loss = 0.00037576310569420457\n",
      "Epoch 680\n",
      "lr = 1.0443303755126414e-06, loss = 0.00014526226732414216\n",
      "Epoch 681\n",
      "lr = 1.0338870717575149e-06, loss = 0.0008443816332146525\n",
      "Epoch 682\n",
      "lr = 1.0235482010399398e-06, loss = 0.007867492735385895\n",
      "Epoch 683\n",
      "lr = 1.0133127190295405e-06, loss = 0.010589048266410828\n",
      "Epoch 684\n",
      "lr = 1.003179591839245e-06, loss = 0.008412426337599754\n",
      "Epoch 685\n",
      "lr = 9.931477959208526e-07, loss = 0.0020440034568309784\n",
      "Epoch 686\n",
      "lr = 9.83216317961644e-07, loss = 0.0003135239239782095\n",
      "Epoch 687\n",
      "lr = 9.733841547820276e-07, loss = 0.005463312845677137\n",
      "Epoch 688\n",
      "lr = 9.636503132342072e-07, loss = 0.00048054830403998494\n",
      "Epoch 689\n",
      "lr = 9.54013810101865e-07, loss = 0.00231272098608315\n",
      "Epoch 690\n",
      "lr = 9.444736720008464e-07, loss = 0.00027798025985248387\n",
      "Epoch 691\n",
      "lr = 9.35028935280838e-07, loss = 0.0001538071082904935\n",
      "Epoch 692\n",
      "lr = 9.256786459280296e-07, loss = 0.0035644222516566515\n",
      "Epoch 693\n",
      "lr = 9.164218594687493e-07, loss = 0.0015562549233436584\n",
      "Epoch 694\n",
      "lr = 9.072576408740618e-07, loss = 0.0040177288465201855\n",
      "Epoch 695\n",
      "lr = 8.981850644653212e-07, loss = 0.01275046169757843\n",
      "Epoch 696\n",
      "lr = 8.89203213820668e-07, loss = 0.00034650706220418215\n",
      "Epoch 697\n",
      "lr = 8.803111816824613e-07, loss = 0.0005912072956562042\n",
      "Epoch 698\n",
      "lr = 8.715080698656366e-07, loss = 0.0003639399947132915\n",
      "Epoch 699\n",
      "lr = 8.627929891669803e-07, loss = 0.0007694399682804942\n",
      "Epoch 700\n",
      "lr = 8.541650592753105e-07, loss = 0.00036608619848266244\n",
      "Epoch 701\n",
      "lr = 8.456234086825574e-07, loss = 0.004343659151345491\n",
      "Epoch 702\n",
      "lr = 8.371671745957318e-07, loss = 0.00018413372163195163\n",
      "Epoch 703\n",
      "lr = 8.287955028497745e-07, loss = 0.015096812509000301\n",
      "Epoch 704\n",
      "lr = 8.205075478212768e-07, loss = 0.0009331796900369227\n",
      "Epoch 705\n",
      "lr = 8.12302472343064e-07, loss = 8.63254172145389e-05\n",
      "Epoch 706\n",
      "lr = 8.041794476196333e-07, loss = 0.00013354100519791245\n",
      "Epoch 707\n",
      "lr = 7.96137653143437e-07, loss = 0.0016025828663259745\n",
      "Epoch 708\n",
      "lr = 7.881762766120026e-07, loss = 0.002955320058390498\n",
      "Epoch 709\n",
      "lr = 7.802945138458826e-07, loss = 0.0074401437304914\n",
      "Epoch 710\n",
      "lr = 7.724915687074238e-07, loss = 0.0002909319009631872\n",
      "Epoch 711\n",
      "lr = 7.647666530203496e-07, loss = 0.0002310076670255512\n",
      "Epoch 712\n",
      "lr = 7.571189864901461e-07, loss = 0.003808818059042096\n",
      "Epoch 713\n",
      "lr = 7.495477966252446e-07, loss = 2.8859391022706404e-05\n",
      "Epoch 714\n",
      "lr = 7.420523186589922e-07, loss = 0.0050557455979287624\n",
      "Epoch 715\n",
      "lr = 7.346317954724022e-07, loss = 0.00012119605526095256\n",
      "Epoch 716\n",
      "lr = 7.272854775176782e-07, loss = 0.0001318277936661616\n",
      "Epoch 717\n",
      "lr = 7.200126227425014e-07, loss = 9.612608846509829e-05\n",
      "Epoch 718\n",
      "lr = 7.128124965150764e-07, loss = 0.023980820551514626\n",
      "Epoch 719\n",
      "lr = 7.056843715499256e-07, loss = 0.0007029249682091177\n",
      "Epoch 720\n",
      "lr = 6.986275278344263e-07, loss = 0.024368468672037125\n",
      "Epoch 721\n",
      "lr = 6.91641252556082e-07, loss = 0.0006951093673706055\n",
      "Epoch 722\n",
      "lr = 6.847248400305212e-07, loss = 0.0008194042602553964\n",
      "Epoch 723\n",
      "lr = 6.77877591630216e-07, loss = 0.01565576158463955\n",
      "Epoch 724\n",
      "lr = 6.710988157139138e-07, loss = 4.583960981108248e-05\n",
      "Epoch 725\n",
      "lr = 6.643878275567747e-07, loss = 0.0034774767700582743\n",
      "Epoch 726\n",
      "lr = 6.577439492812069e-07, loss = 0.001670554280281067\n",
      "Epoch 727\n",
      "lr = 6.511665097883948e-07, loss = 0.00028429049416445196\n",
      "Epoch 728\n",
      "lr = 6.446548446905108e-07, loss = 0.0002563830348663032\n",
      "Epoch 729\n",
      "lr = 6.382082962436057e-07, loss = 0.00018461943545844406\n",
      "Epoch 730\n",
      "lr = 6.318262132811697e-07, loss = 0.003249039174988866\n",
      "Epoch 731\n",
      "lr = 6.25507951148358e-07, loss = 0.006773956120014191\n",
      "Epoch 732\n",
      "lr = 6.192528716368744e-07, loss = 0.0019872041884809732\n",
      "Epoch 733\n",
      "lr = 6.130603429205057e-07, loss = 0.0003950055397581309\n",
      "Epoch 734\n",
      "lr = 6.069297394913007e-07, loss = 0.0013059036573395133\n",
      "Epoch 735\n",
      "lr = 6.008604420963876e-07, loss = 0.0003322541597299278\n",
      "Epoch 736\n",
      "lr = 5.948518376754238e-07, loss = 0.00046150683192536235\n",
      "Epoch 737\n",
      "lr = 5.889033192986695e-07, loss = 1.0796497008414008e-05\n",
      "Epoch 738\n",
      "lr = 5.830142861056828e-07, loss = 0.0005134231760166585\n",
      "Epoch 739\n",
      "lr = 5.77184143244626e-07, loss = 0.0012554294662550092\n",
      "Epoch 740\n",
      "lr = 5.714123018121798e-07, loss = 0.00013792946992907673\n",
      "Epoch 741\n",
      "lr = 5.656981787940579e-07, loss = 0.0010534169850870967\n",
      "Epoch 742\n",
      "lr = 5.600411970061174e-07, loss = 0.013971899636089802\n",
      "Epoch 743\n",
      "lr = 5.544407850360562e-07, loss = 0.010970237664878368\n",
      "Epoch 744\n",
      "lr = 5.488963771856956e-07, loss = 0.02636859193444252\n",
      "Epoch 745\n",
      "lr = 5.434074134138387e-07, loss = 0.0037426711060106754\n",
      "Epoch 746\n",
      "lr = 5.379733392797003e-07, loss = 0.00015734239423181862\n",
      "Epoch 747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 5.325936058869033e-07, loss = 0.006179916672408581\n",
      "Epoch 748\n",
      "lr = 5.272676698280343e-07, loss = 0.002481352537870407\n",
      "Epoch 749\n",
      "lr = 5.219949931297539e-07, loss = 0.0006078387959860265\n",
      "Epoch 750\n",
      "lr = 5.167750431984564e-07, loss = 0.00019535073079168797\n",
      "Epoch 751\n",
      "lr = 5.116072927664718e-07, loss = 0.014575840905308723\n",
      "Epoch 752\n",
      "lr = 5.064912198388072e-07, loss = 0.0002486024459358305\n",
      "Epoch 753\n",
      "lr = 5.014263076404191e-07, loss = 0.0001640334667172283\n",
      "Epoch 754\n",
      "lr = 4.964120445640149e-07, loss = 0.0014652508543804288\n",
      "Epoch 755\n",
      "lr = 4.914479241183747e-07, loss = 0.00012022387090837583\n",
      "Epoch 756\n",
      "lr = 4.865334448771909e-07, loss = 0.012760641984641552\n",
      "Epoch 757\n",
      "lr = 4.81668110428419e-07, loss = 0.00030557485297322273\n",
      "Epoch 758\n",
      "lr = 4.768514293241349e-07, loss = 0.03450407832860947\n",
      "Epoch 759\n",
      "lr = 4.7208291503089353e-07, loss = 0.008678693324327469\n",
      "Epoch 760\n",
      "lr = 4.673620858805846e-07, loss = 0.013861749321222305\n",
      "Epoch 761\n",
      "lr = 4.626884650217787e-07, loss = 0.013694200664758682\n",
      "Epoch 762\n",
      "lr = 4.5806158037156093e-07, loss = 0.0008252841653302312\n",
      "Epoch 763\n",
      "lr = 4.5348096456784534e-07, loss = 0.0006895349943079054\n",
      "Epoch 764\n",
      "lr = 4.4894615492216687e-07, loss = 0.00021730089792981744\n",
      "Epoch 765\n",
      "lr = 4.444566933729452e-07, loss = 0.00028308475157245994\n",
      "Epoch 766\n",
      "lr = 4.400121264392157e-07, loss = 0.01959356851875782\n",
      "Epoch 767\n",
      "lr = 4.3561200517482357e-07, loss = 0.0006605175440199673\n",
      "Epoch 768\n",
      "lr = 4.3125588512307533e-07, loss = 0.0006343730492517352\n",
      "Epoch 769\n",
      "lr = 4.2694332627184456e-07, loss = 0.00043793913209810853\n",
      "Epoch 770\n",
      "lr = 4.226738930091261e-07, loss = 0.0035032981541007757\n",
      "Epoch 771\n",
      "lr = 4.1844715407903484e-07, loss = 0.005137407220900059\n",
      "Epoch 772\n",
      "lr = 4.142626825382445e-07, loss = 0.0003507592773530632\n",
      "Epoch 773\n",
      "lr = 4.1012005571286204e-07, loss = 0.004426863510161638\n",
      "Epoch 774\n",
      "lr = 4.060188551557334e-07, loss = 0.0001367390650557354\n",
      "Epoch 775\n",
      "lr = 4.0195866660417607e-07, loss = 0.009972791187465191\n",
      "Epoch 776\n",
      "lr = 3.9793907993813433e-07, loss = 0.00028793999808840454\n",
      "Epoch 777\n",
      "lr = 3.93959689138753e-07, loss = 3.5998094972455874e-05\n",
      "Epoch 778\n",
      "lr = 3.9002009224736545e-07, loss = 0.008869259618222713\n",
      "Epoch 779\n",
      "lr = 3.861198913248918e-07, loss = 0.007153788581490517\n",
      "Epoch 780\n",
      "lr = 3.822586924116429e-07, loss = 7.549353176727891e-05\n",
      "Epoch 781\n",
      "lr = 3.7843610548752643e-07, loss = 0.00021112854301463813\n",
      "Epoch 782\n",
      "lr = 3.7465174443265115e-07, loss = 0.035288255661726\n",
      "Epoch 783\n",
      "lr = 3.7090522698832463e-07, loss = 0.004543142858892679\n",
      "Epoch 784\n",
      "lr = 3.671961747184414e-07, loss = 0.0025817856658250093\n",
      "Epoch 785\n",
      "lr = 3.6352421297125694e-07, loss = 0.0007805274799466133\n",
      "Epoch 786\n",
      "lr = 3.598889708415444e-07, loss = 0.007789010647684336\n",
      "Epoch 787\n",
      "lr = 3.5629008113312896e-07, loss = 5.415097621153109e-05\n",
      "Epoch 788\n",
      "lr = 3.5272718032179766e-07, loss = 0.0011163171147927642\n",
      "Epoch 789\n",
      "lr = 3.491999085185797e-07, loss = 0.010391227900981903\n",
      "Epoch 790\n",
      "lr = 3.4570790943339386e-07, loss = 0.0014726456720381975\n",
      "Epoch 791\n",
      "lr = 3.422508303390599e-07, loss = 0.009785182774066925\n",
      "Epoch 792\n",
      "lr = 3.388283220356693e-07, loss = 0.00017439085058867931\n",
      "Epoch 793\n",
      "lr = 3.3544003881531265e-07, loss = 0.0006073391414247453\n",
      "Epoch 794\n",
      "lr = 3.320856384271595e-07, loss = 0.0007822384941391647\n",
      "Epoch 795\n",
      "lr = 3.2876478204288795e-07, loss = 0.00014679550076834857\n",
      "Epoch 796\n",
      "lr = 3.254771342224591e-07, loss = 0.0004899338819086552\n",
      "Epoch 797\n",
      "lr = 3.222223628802345e-07, loss = 0.0027116420678794384\n",
      "Epoch 798\n",
      "lr = 3.1900013925143216e-07, loss = 0.00040048203663900495\n",
      "Epoch 799\n",
      "lr = 3.158101378589178e-07, loss = 0.021615156903862953\n",
      "Epoch 800\n",
      "lr = 3.126520364803286e-07, loss = 0.000623327330686152\n",
      "Epoch 801\n",
      "lr = 3.095255161155253e-07, loss = 0.0015737132634967566\n",
      "Epoch 802\n",
      "lr = 3.064302609543701e-07, loss = 0.0005261066253297031\n",
      "Epoch 803\n",
      "lr = 3.0336595834482637e-07, loss = 0.014432171359658241\n",
      "Epoch 804\n",
      "lr = 3.003322987613781e-07, loss = 0.0007270744536072016\n",
      "Epoch 805\n",
      "lr = 2.973289757737643e-07, loss = 0.0006147997919470072\n",
      "Epoch 806\n",
      "lr = 2.943556860160267e-07, loss = 0.00018921501759905368\n",
      "Epoch 807\n",
      "lr = 2.9141212915586643e-07, loss = 0.0015391813358291984\n",
      "Epoch 808\n",
      "lr = 2.8849800786430775e-07, loss = 0.0022352784872055054\n",
      "Epoch 809\n",
      "lr = 2.8561302778566467e-07, loss = 0.00029482567333616316\n",
      "Epoch 810\n",
      "lr = 2.82756897507808e-07, loss = 0.0007866026717238128\n",
      "Epoch 811\n",
      "lr = 2.7992932853272993e-07, loss = 0.007490672171115875\n",
      "Epoch 812\n",
      "lr = 2.771300352474026e-07, loss = 0.002554448787122965\n",
      "Epoch 813\n",
      "lr = 2.743587348949286e-07, loss = 0.0002276814338983968\n",
      "Epoch 814\n",
      "lr = 2.716151475459793e-07, loss = 0.0019172275206074119\n",
      "Epoch 815\n",
      "lr = 2.688989960705195e-07, loss = 0.0017438118811696768\n",
      "Epoch 816\n",
      "lr = 2.662100061098143e-07, loss = 0.0006235468899831176\n",
      "Epoch 817\n",
      "lr = 2.635479060487162e-07, loss = 0.0004547678981907666\n",
      "Epoch 818\n",
      "lr = 2.6091242698822904e-07, loss = 0.0013155073393136263\n",
      "Epoch 819\n",
      "lr = 2.5830330271834677e-07, loss = 0.028673233464360237\n",
      "Epoch 820\n",
      "lr = 2.557202696911633e-07, loss = 0.012618932873010635\n",
      "Epoch 821\n",
      "lr = 2.531630669942517e-07, loss = 0.00777690252289176\n",
      "Epoch 822\n",
      "lr = 2.5063143632430915e-07, loss = 0.03958014398813248\n",
      "Epoch 823\n",
      "lr = 2.4812512196106605e-07, loss = 8.058189996518195e-05\n",
      "Epoch 824\n",
      "lr = 2.456438707414554e-07, loss = 0.0008212667307816446\n",
      "Epoch 825\n",
      "lr = 2.431874320340408e-07, loss = 0.000973165559116751\n",
      "Epoch 826\n",
      "lr = 2.407555577137004e-07, loss = 0.004871058277785778\n",
      "Epoch 827\n",
      "lr = 2.383480021365634e-07, loss = 0.002156422706320882\n",
      "Epoch 828\n",
      "lr = 2.3596452211519777e-07, loss = 0.0006157903699204326\n",
      "Epoch 829\n",
      "lr = 2.336048768940458e-07, loss = 0.0005285725928843021\n",
      "Epoch 830\n",
      "lr = 2.3126882812510533e-07, loss = 0.0008716984884813428\n",
      "Epoch 831\n",
      "lr = 2.2895613984385427e-07, loss = 0.0077516972087323666\n",
      "Epoch 832\n",
      "lr = 2.2666657844541572e-07, loss = 0.0009214875171892345\n",
      "Epoch 833\n",
      "lr = 2.2439991266096156e-07, loss = 0.00041347104706801474\n",
      "Epoch 834\n",
      "lr = 2.2215591353435195e-07, loss = 0.004030600655823946\n",
      "Epoch 835\n",
      "lr = 2.1993435439900843e-07, loss = 0.0013782328460365534\n",
      "Epoch 836\n",
      "lr = 2.1773501085501834e-07, loss = 0.004012939054518938\n",
      "Epoch 837\n",
      "lr = 2.1555766074646815e-07, loss = 0.015691637992858887\n",
      "Epoch 838\n",
      "lr = 2.1340208413900346e-07, loss = 0.004492176230996847\n",
      "Epoch 839\n",
      "lr = 2.1126806329761342e-07, loss = 0.0036639950703829527\n",
      "Epoch 840\n",
      "lr = 2.0915538266463727e-07, loss = 0.0006960279424674809\n",
      "Epoch 841\n",
      "lr = 2.070638288379909e-07, loss = 0.00044536456698551774\n",
      "Epoch 842\n",
      "lr = 2.0499319054961098e-07, loss = 0.0014711349504068494\n",
      "Epoch 843\n",
      "lr = 2.0294325864411485e-07, loss = 0.017775818705558777\n",
      "Epoch 844\n",
      "lr = 2.009138260576737e-07, loss = 0.0001328561920672655\n",
      "Epoch 845\n",
      "lr = 1.9890468779709696e-07, loss = 0.00019174900080543011\n",
      "Epoch 846\n",
      "lr = 1.96915640919126e-07, loss = 0.0031224030535668135\n",
      "Epoch 847\n",
      "lr = 1.9494648450993473e-07, loss = 0.0015605328371748328\n",
      "Epoch 848\n",
      "lr = 1.9299701966483537e-07, loss = 0.004209158476442099\n",
      "Epoch 849\n",
      "lr = 1.9106704946818702e-07, loss = 0.0182614978402853\n",
      "Epoch 850\n",
      "lr = 1.8915637897350516e-07, loss = 0.0012688960414379835\n",
      "Epoch 851\n",
      "lr = 1.872648151837701e-07, loss = 0.004046497400850058\n",
      "Epoch 852\n",
      "lr = 1.853921670319324e-07, loss = 0.0004445949452929199\n",
      "Epoch 853\n",
      "lr = 1.8353824536161306e-07, loss = 0.0008492367924191058\n",
      "Epoch 854\n",
      "lr = 1.8170286290799692e-07, loss = 0.0005624137120321393\n",
      "Epoch 855\n",
      "lr = 1.7988583427891694e-07, loss = 0.0005677468725480139\n",
      "Epoch 856\n",
      "lr = 1.7808697593612778e-07, loss = 0.0021668800618499517\n",
      "Epoch 857\n",
      "lr = 1.763061061767665e-07, loss = 0.0012006777105852962\n",
      "Epoch 858\n",
      "lr = 1.745430451149988e-07, loss = 0.0005264884093776345\n",
      "Epoch 859\n",
      "lr = 1.7279761466384882e-07, loss = 0.00012000597052974626\n",
      "Epoch 860\n",
      "lr = 1.7106963851721033e-07, loss = 0.00016791468078736216\n",
      "Epoch 861\n",
      "lr = 1.6935894213203822e-07, loss = 0.003181389532983303\n",
      "Epoch 862\n",
      "lr = 1.6766535271071784e-07, loss = 0.00037384347524493933\n",
      "Epoch 863\n",
      "lr = 1.6598869918361066e-07, loss = 0.0003826648462563753\n",
      "Epoch 864\n",
      "lr = 1.6432881219177455e-07, loss = 0.00011430997255956754\n",
      "Epoch 865\n",
      "lr = 1.6268552406985682e-07, loss = 0.0007558518555015326\n",
      "Epoch 866\n",
      "lr = 1.6105866882915825e-07, loss = 0.0003611593565437943\n",
      "Epoch 867\n",
      "lr = 1.5944808214086665e-07, loss = 0.00018048913625534624\n",
      "Epoch 868\n",
      "lr = 1.57853601319458e-07, loss = 0.018516365438699722\n",
      "Epoch 869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 1.562750653062634e-07, loss = 0.007210100535303354\n",
      "Epoch 870\n",
      "lr = 1.5471231465320078e-07, loss = 0.0020708669908344746\n",
      "Epoch 871\n",
      "lr = 1.5316519150666876e-07, loss = 0.0014120175037533045\n",
      "Epoch 872\n",
      "lr = 1.5163353959160208e-07, loss = 0.000126147351693362\n",
      "Epoch 873\n",
      "lr = 1.5011720419568604e-07, loss = 0.00024854339426383376\n",
      "Epoch 874\n",
      "lr = 1.4861603215372917e-07, loss = 6.235930050024763e-05\n",
      "Epoch 875\n",
      "lr = 1.4712987183219187e-07, loss = 0.0030258132610470057\n",
      "Epoch 876\n",
      "lr = 1.4565857311386995e-07, loss = 5.230757960816845e-05\n",
      "Epoch 877\n",
      "lr = 1.4420198738273124e-07, loss = 0.0006994727882556617\n",
      "Epoch 878\n",
      "lr = 1.4275996750890393e-07, loss = 0.0021475900430232286\n",
      "Epoch 879\n",
      "lr = 1.4133236783381488e-07, loss = 0.009308293461799622\n",
      "Epoch 880\n",
      "lr = 1.3991904415547673e-07, loss = 0.008734138682484627\n",
      "Epoch 881\n",
      "lr = 1.3851985371392195e-07, loss = 5.675750435329974e-05\n",
      "Epoch 882\n",
      "lr = 1.3713465517678272e-07, loss = 0.0004330329247750342\n",
      "Epoch 883\n",
      "lr = 1.357633086250149e-07, loss = 0.0009229176794178784\n",
      "Epoch 884\n",
      "lr = 1.3440567553876473e-07, loss = 0.0020265046041458845\n",
      "Epoch 885\n",
      "lr = 1.330616187833771e-07, loss = 0.0009433813393115997\n",
      "Epoch 886\n",
      "lr = 1.3173100259554332e-07, loss = 0.0005236441502347589\n",
      "Epoch 887\n",
      "lr = 1.3041369256958787e-07, loss = 0.011525489389896393\n",
      "Epoch 888\n",
      "lr = 1.29109555643892e-07, loss = 0.0007290495559573174\n",
      "Epoch 889\n",
      "lr = 1.2781846008745307e-07, loss = 0.00014586008910555393\n",
      "Epoch 890\n",
      "lr = 1.2654027548657854e-07, loss = 0.00038530922029167414\n",
      "Epoch 891\n",
      "lr = 1.2527487273171275e-07, loss = 0.00018547607760410756\n",
      "Epoch 892\n",
      "lr = 1.2402212400439562e-07, loss = 0.0002205886848969385\n",
      "Epoch 893\n",
      "lr = 1.2278190276435166e-07, loss = 0.006009034346789122\n",
      "Epoch 894\n",
      "lr = 1.2155408373670814e-07, loss = 0.00039366917917504907\n",
      "Epoch 895\n",
      "lr = 1.2033854289934105e-07, loss = 0.00042435075738467276\n",
      "Epoch 896\n",
      "lr = 1.1913515747034764e-07, loss = 0.000549609656445682\n",
      "Epoch 897\n",
      "lr = 1.1794380589564416e-07, loss = 0.0006127775413915515\n",
      "Epoch 898\n",
      "lr = 1.1676436783668772e-07, loss = 0.0017627781489863992\n",
      "Epoch 899\n",
      "lr = 1.1559672415832084e-07, loss = 0.0007704723393544555\n",
      "Epoch 900\n",
      "lr = 1.1444075691673763e-07, loss = 0.003365946700796485\n",
      "Epoch 901\n",
      "lr = 1.1329634934757025e-07, loss = 0.01395381148904562\n",
      "Epoch 902\n",
      "lr = 1.1216338585409455e-07, loss = 3.8348323869286105e-05\n",
      "Epoch 903\n",
      "lr = 1.110417519955536e-07, loss = 0.0005038537201471627\n",
      "Epoch 904\n",
      "lr = 1.0993133447559807e-07, loss = 0.03570600599050522\n",
      "Epoch 905\n",
      "lr = 1.0883202113084208e-07, loss = 0.00477369362488389\n",
      "Epoch 906\n",
      "lr = 1.0774370091953366e-07, loss = 0.0013300834689289331\n",
      "Epoch 907\n",
      "lr = 1.0666626391033832e-07, loss = 0.0041250926442444324\n",
      "Epoch 908\n",
      "lr = 1.0559960127123493e-07, loss = 0.0003539952449500561\n",
      "Epoch 909\n",
      "lr = 1.0454360525852258e-07, loss = 0.0006211926229298115\n",
      "Epoch 910\n",
      "lr = 1.0349816920593736e-07, loss = 0.0001791472896002233\n",
      "Epoch 911\n",
      "lr = 1.0246318751387799e-07, loss = 0.00010968742572003976\n",
      "Epoch 912\n",
      "lr = 1.0143855563873921e-07, loss = 0.00244384934194386\n",
      "Epoch 913\n",
      "lr = 1.0042417008235182e-07, loss = 0.0007431881967931986\n",
      "Epoch 914\n",
      "lr = 9.94199283815283e-08, loss = 0.006528560537844896\n",
      "Epoch 915\n",
      "lr = 9.842572909771301e-08, loss = 0.00014443023246712983\n",
      "Epoch 916\n",
      "lr = 9.744147180673588e-08, loss = 8.435955533059314e-05\n",
      "Epoch 917\n",
      "lr = 9.646705708866852e-08, loss = 0.00046037169522605836\n",
      "Epoch 918\n",
      "lr = 9.550238651778183e-08, loss = 0.00026263348991051316\n",
      "Epoch 919\n",
      "lr = 9.454736265260401e-08, loss = 0.011096801608800888\n",
      "Epoch 920\n",
      "lr = 9.360188902607796e-08, loss = 0.020481524989008904\n",
      "Epoch 921\n",
      "lr = 9.266587013581718e-08, loss = 0.00018876891408581287\n",
      "Epoch 922\n",
      "lr = 9.1739211434459e-08, loss = 0.0004653635260183364\n",
      "Epoch 923\n",
      "lr = 9.08218193201144e-08, loss = 0.0009674476459622383\n",
      "Epoch 924\n",
      "lr = 8.991360112691326e-08, loss = 0.0011131870560348034\n",
      "Epoch 925\n",
      "lr = 8.901446511564414e-08, loss = 0.0014131604693830013\n",
      "Epoch 926\n",
      "lr = 8.81243204644877e-08, loss = 0.007400969043374062\n",
      "Epoch 927\n",
      "lr = 8.724307725984281e-08, loss = 0.0009235371253453195\n",
      "Epoch 928\n",
      "lr = 8.637064648724439e-08, loss = 0.0006165510858409107\n",
      "Epoch 929\n",
      "lr = 8.550694002237195e-08, loss = 0.006026470568031073\n",
      "Epoch 930\n",
      "lr = 8.465187062214823e-08, loss = 0.001929318648763001\n",
      "Epoch 931\n",
      "lr = 8.380535191592675e-08, loss = 0.00047765221097506583\n",
      "Epoch 932\n",
      "lr = 8.296729839676748e-08, loss = 0.0010241279378533363\n",
      "Epoch 933\n",
      "lr = 8.21376254127998e-08, loss = 0.0003253899631090462\n",
      "Epoch 934\n",
      "lr = 8.131624915867181e-08, loss = 0.0008249581442214549\n",
      "Epoch 935\n",
      "lr = 8.05030866670851e-08, loss = 0.0006345156580209732\n",
      "Epoch 936\n",
      "lr = 7.969805580041424e-08, loss = 0.00011557654943317175\n",
      "Epoch 937\n",
      "lr = 7.89010752424101e-08, loss = 1.841431003413163e-05\n",
      "Epoch 938\n",
      "lr = 7.8112064489986e-08, loss = 0.0003075968415942043\n",
      "Epoch 939\n",
      "lr = 7.733094384508615e-08, loss = 0.0031086995732039213\n",
      "Epoch 940\n",
      "lr = 7.655763440663528e-08, loss = 0.012822863645851612\n",
      "Epoch 941\n",
      "lr = 7.579205806256893e-08, loss = 0.00034801577567122877\n",
      "Epoch 942\n",
      "lr = 7.503413748194323e-08, loss = 0.002203427953645587\n",
      "Epoch 943\n",
      "lr = 7.42837961071238e-08, loss = 0.008534192107617855\n",
      "Epoch 944\n",
      "lr = 7.354095814605256e-08, loss = 0.0034620501101017\n",
      "Epoch 945\n",
      "lr = 7.280554856459203e-08, loss = 0.013210557401180267\n",
      "Epoch 946\n",
      "lr = 7.207749307894611e-08, loss = 0.002144134370610118\n",
      "Epoch 947\n"
     ]
    }
   ],
   "source": [
    "# net = CNN1(10)\n",
    "# lr = 0.001\n",
    "# losses = []\n",
    "\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = optim.Adam(net.parameters(), lr = lr)\n",
    "\n",
    "\n",
    "# X = Variable(torch.FloatTensor(X_train[:,:,np.newaxis,:]))\n",
    "# y = Variable(torch.LongTensor(y_train.astype(int)))\n",
    "\n",
    "print(X.size(0))\n",
    "### train model\n",
    "batch_size = 200\n",
    "\n",
    "num_epochs = 502\n",
    "for epoch in range(num_epochs, 1000):\n",
    "    print('Epoch {}'.format(epoch))\n",
    "    \n",
    "    permutation = torch.randperm(X.size()[0]) \n",
    "    # type(permutation) = torch.LongTensor\n",
    "    lr = update_lr(optimizer, epoch, lr)\n",
    "    for i in range(0, X.size()[0], batch_size):\n",
    "        \n",
    "        idxs = permutation[i:i + batch_size]\n",
    "        batch_input, batch_target = X[idxs], y[idxs]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_output = net(batch_input)\n",
    "\n",
    "        batch_loss = criterion(batch_output, batch_target)\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(batch_loss.data[0])\n",
    "        \n",
    "    print('lr = {}, loss = {}'.format(lr, losses[-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### test model\n",
    "XX = Variable(torch.FloatTensor(X_val[:,:,np.newaxis,:]))\n",
    "yy = Variable(torch.LongTensor(y_val.astype(int)))\n",
    "\n",
    "outputs  = net(XX)\n",
    "_, predicted = torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 7 7 6 6 6 6 6 1 7 7 6 1 9 4 6 1 4 9 3 7 9 8 1 1 3 3 3 3 6 7 7 0 0 0 2\n",
      " 2 4 5 0 1 0 0 7 6 5 4 2 2 3 4 4 0 0 1 6 0 9 9 1 7 8 4 2 2 2 9 6 3 2 8 7 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6351351351351351"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(predicted.numpy().ravel())\n",
    "\n",
    "accuracy_score(yy.data.numpy().ravel(), predicted.numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA, TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npz_path = \"/home/julia/DeepVoice_data/ESC/esc10/wav16.npz\"\n",
    "path_to_scatnet = '/home/julia/Downloads/scatnet-master/'\n",
    "\n",
    "\n",
    "np_arr_batch = val_sounds\n",
    "N = np_arr_batch.shape[1]\n",
    "T = 1024\n",
    "\n",
    "S_table_batch, time_count, coeffs_count = scattering.get_scattering_coefficients(path_to_scatnet,\n",
    "                                                                                 np_arr_batch,\n",
    "                                                                                 N, T, M = 2,\n",
    "                                                                                 Q = [8,1],\n",
    "                                                                                 renorm = True,\n",
    "                                                                                 log = True)\n",
    "\n",
    "print(S_table_batch.shape, time_count, coeffs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = S_table_batch.shape\n",
    "S_table_batch_flat = S_table_batch.reshape((shape[0], shape[1]*shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=50, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components = 50)\n",
    "pca.fit(S_table_batch_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33862477 0.28488741 0.07760982 0.05862403 0.04048566 0.02161015\n",
      " 0.01964886 0.01649891 0.01447027 0.0104535  0.00973113 0.00778712\n",
      " 0.00665288 0.00609942 0.00555213 0.0050645  0.00462347 0.00426475\n",
      " 0.00423024 0.00381197 0.00363839 0.0033333  0.0031319  0.0027903\n",
      " 0.00264634 0.00257276 0.00227847 0.00215672 0.00204386 0.00190188\n",
      " 0.00188738 0.00165441 0.0016048  0.00156279 0.00143674 0.00137723\n",
      " 0.00126963 0.00125666 0.00117938 0.00107962 0.00103583 0.00101781\n",
      " 0.00096049 0.00092818 0.0009103  0.00088216 0.00085576 0.00083924\n",
      " 0.00078075 0.00074144]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 50)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_table_batch_pca = pca.transform(S_table_batch_flat)\n",
    "S_table_batch_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S_table_batch_tsne = TSNE(n_components=2).fit_transform(S_table_batch_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHaVJREFUeJzt3W1sXNd95/Hvf4YcYSVVI8a2JEseaazaWtiGAZIeU1rt\nZh+QQHGLIkoXduFu4DAPtrFoirZ+09hrZPsiMJBskQTwNptGUu0qhhvDdRvYi82D46yLaqGV5JHI\njSIrVhhmmLFsSbZLjSoJ4Aznnn0xl/JEpkSKc+/ch/l9AILDe4d3zgGl+c05555zzDmHiIj0tkzU\nBRARkegpDERERGEgIiIKAxERQWEgIiIoDEREhADCwMwKZvaqmb1uZsfM7I/94x8ysx+Z2c/97wNt\nv/OYmU2Y2Rtm9rFOyyAiIp2xTucZmNmNwI3OuSNm9hvAYeATwKeBf3LOfdnMHgUGnHNfMLPbge8A\nI8B64BVgi3Ou2VFBRERkyTpuGTjn3nbOHfEf/zNwHNgA7AT2+k/bSysg8I8/55ybcc79EpigFQwi\nIhKRviAvZmZFYAg4CKx1zr3tnzoFrPUfbwAOtP3am/6xq7r++utdsVgMqqgiIj3h8OHD7zrnbljo\neYGFgZmtBP4O+BPn3Dkzu3TOOefM7Jr7o8zsYeBhgI0bN1Iul4MqrohITzCzqcU8L5C7icysn1YQ\nPOuc+3v/8Gl/PGFuXOGMf/wkUGj79Zv8Yx/gnNvlnCs550o33LBgsImIyBIFcTeRAX8FHHfOfa3t\n1EvAqP94FHix7fj9ZrbMzG4GbgUOdVoOERFZuiC6if418ABw1MzG/WP/Bfgy8LyZfQ6YAn4PwDl3\nzMyeB14HZoHP604iEZFodRwGzrn/A9gVTn/kCr/zBPBEp68tIiLB0AxkERFRGIiIiMJARGJs/Mw4\ne47uYfzM+MJPlo4EOulMRCQo42fGeejlh6g36+SyOXbv2M3gmsGoi5VaahmISCyVT5epN+t4eDS8\nBuXTmnQaJoWBiMRSaW2JXDZH1rL0Z/oprS1FXaRUUzeRiFyTmalzzEzWWLY5z7JNq0J7ncE1g+ze\nsZvy6TKltSV1EYVMYSAiizYzdY539xzFzXpYX4brH7wz9EBQCHSHuolEZNFmJmu4WQ8cuFmPmcla\n1EWSgCgMRGTRlm3OY30ZMLC+DMs256MukgRE3UQismjLNq3i+gfv7MqYgXSXwkBErsmyTasUAimk\nbiIREVEYiIiIwkBERFAYiCRKuXaBJ6dOU65diLookjIaQBZJiHLtAveOT9DwHP0Z44XBWyjlV0Rd\nLEkJtQxEEmL/2fM0PEcTaHiO/WfPX/G5cW5BzEyd49yrVWamziXi2tVqlX379lGtVgO7ZhypZSCS\nENtXr6Q/Y+C3DLavXjnv8+LcgghzOYswrl2tVtm7dy/NZpNsNsvo6CiFQiGQ8saNWgYiCVHKr+CF\nwVv4wuYbr/oGfy0tiG4LczmLMK5dqVRoNps452g2m1Qqlc4LGlNqGYgkSCm/YsFP+YttQURhbjmL\nuU/vQS5nEca1i8Ui2Wz2UsugWCx2XtCYMudc1GVYlFKp5MplbW4hshjl2gX2nz3P9tUrY9NFNCfM\nJbDDuHa1WqVSqVAsFrveRRTE39HMDjvnFtwMQmEgIhJDQY39LDYMNGYgIhJD3R77URiIiMTQ3NhP\nFroy9qMBZBFZtG5teSnv3z3WrbEfhYGILEq3t7yUxd09FhR1E4nIomjLy3RTGIjIomjLy3RTN5GI\nLIq2vEw3hYGILJq2vEwvdROJSDxVD8G+r7a+S+jUMhCR+Kkegr0fh2YdsjkYfQkKI1GXKtXUMhCR\n+KnsawWBa7a+V/YFdunxM+PsObqH8TPjgV0zDdQyEJH4KX641SKYaxkUPxzIZcfPjPPQyw9Rb9bJ\nZXPs3rGbwTWDgVw76RQGIhI/hZFW11BlXysIAuoiKp8uU2/W8fBoeA3Kp8sKA5/CQCQAtdoRpqcP\nMjCwlXx+OOripENhJPBxgtLaErlsjobXoD/TT2ntgot59gyFgUiHarUjHBl7AM+rk8nkGB56RoEQ\nU4NrBtm9Yzfl02VKa0uBtAqi3O8gSAoDkQ5NTx/E8+qAh+c1mJ4+qDCIscE1g4F1DaVpj+RA7iYy\ns6fM7IyZ/bTt2IfM7Edm9nP/+0DbucfMbMLM3jCzjwVRBpGoDAxsJZPJAVkymX4GBrZGXSTpkjTt\nkRzUraV/Ddxz2bFHgR87524Ffuz/jJndDtwP3OH/zv8ws2xA5RC5pFY7QqXyTWq1I6G+Tj4/zPDQ\nM/zm5kcS2UU0M3WOc69WmZk6F3VREmduj2QzS/weyYF0Eznn/tHMipcd3gn8e//xXuAfgC/4x59z\nzs0AvzSzCWAE+L9BlEUEut+Pn88PJy4EoAvLUlcPBX5HUJwUCgVGR0c1ZrCAtc65t/3Hp4C1/uMN\nwIG2573pH/sAM3sYeBhg48aNIRVT0kj9+Isz37LUgYVBj8wiLhQKiQ6BOV2Zgeycc4Bbwu/tcs6V\nnHOlG264IYSSSVqluR//8NQ033h1gsNT0x1fK9RlqUOcRSzBC7NlcNrMbnTOvW1mNwJn/OMngfYY\nvck/JhKYuX78tN37f3hqmk/uOUB91iPXl+HZB7dx16aBhX/xCkJdljqkWcQSjjDD4CVgFPiy//3F\ntuN/Y2ZfA9YDtwJallACl9R+/Ks5MPke9VkPz0Fj1uPA5HsdhQGEuCx1SLOILzd+ZjzQeQO9KpAw\nMLPv0Bosvt7M3gT+jFYIPG9mnwOmgN8DcM4dM7PngdeBWeDzzrlmEOUQSbttm68j15ehMevR35dh\n2+broi7S1YUwi7id1hoKTlB3E/3+FU595ArPfwJ4IojXFukld20a4NkHt3Fg8j22bb6u41ZB0mmt\noeBoBrJIwty1aaDnQ2CO1hoKjsJARBIrjLWGepXCQEQSLbC1hlI+QW4hCgMRkR6ZIHc12vZSREQT\n5BQGIhID1UOw76ut71GYmyBn2Z6dIKduIhGJVhy6aLo0QS7OFAYiEq35umiieDMOY4JcggalFQYi\nEq20rmEUhxbPNVAYiETo8NR0ImcTXxwb4+Kh11g+cjfLh4Y6u1hau2ji0uJZJIWBSESCXoH0aoJ8\n8744NsavPvNZXL2O5XJsfPqpYAIhxm+US5KwFo/CQGSRgv4UH8YKpPMJ+s374qHXcPU6eB6u0WiF\nTKdhkEYJa/EoDEQWIYxP8d1agTToN+/lI3djuRyu0cD6+1k+cneApe1M7JazTlCLR2EgsghhfIrv\n1gqkQb95Lx8aYuPTTwU3ZhAQLWfdGYWByCKE9Sm+GyuQhvHmvXxoKDYhMEfLWXdGYSCyCEnfRyCO\nb95B03LWnbHWXvXxVyqVXLlcjroYIhJjsRsziAEzO+ycWzAZ1TIQkdQIbDnrHqSF6kRERGEgIiIK\nAxGJQtRLVssHaMxARLorYQu49Qq1DESku2K+q9j4mXH2HN3D+JnxqIvSVWoZSOKcmqxx8sQ0G7YM\nsG5zPuriyLWK8QJuvTyLWWEgiXJqssaLXx+jOeuR7cuw85EhBULSxHgBt16exawwkEQ5eWKa5qyH\nc9Bsepw8MZ2oMCjXLrD/7Hm2r15JKb8ikGsmck+EmC7g1suzmBUGkigbtgyQ7cvQbHpksxk2bEnI\nmx+tILh3fIKG5+jPGC8M3tJxIHRjT4RAN7KJucE1g+zesbsnZzErDCRR1m3Os/ORoUSOGew/e56G\n52gCeI79Z893HAZh74kQykY2Mders5gVBpI46zbnExUCc7avXkl/xsBvGWxfvbLja4a9J0KsN7JJ\n0GbzSaAwEOmSUn4FLwzeEuiYQdirqcZ2IxvNVQicwkDkGnQ6AFzKrwhs4HhOmHsixHUjm6RtNp8E\nCgORRQpjADgJYrkXQoznKiSVwkBkkcIYAJYlivFchaRSGIgsUhgDwNKBmM5VSCqFgcgihTEALBIX\nCgORaxDGALBIHGjVUhERURiIiEiEYWBm95jZG2Y2YWaPRlUOERGJKAzMLAt8A/gt4Hbg983s9ijK\nIiIi0bUMRoAJ59ykc64OPAfsjKgsIrFxeGqab7w6weGp6aiLIj0mqruJNgDVtp/fBLZGVBaRWOjG\nctQiVxLrAWQze9jMymZWfuedd6IujiTcWyeOc/C7z/PWieNRF2Ve8y1HHZaLY2O8+61dXBwbC+01\nJFmiahmcBAptP9/kH/s1zrldwC6AUqnkulM0SaO3Thznb7/0OM3ZWbJ9fdz3xSdYv+W2qIv1a8Je\njnpOHPYoSOTubCkXVRi8BtxqZjfTCoH7gf8UUVmkB1SPHaU5O4vzPJqzs1SPHY1dGIS9HPWcqPco\nUHdYPEUSBs65WTP7Q+CHQBZ4yjl3LIqySG8o3HEn2b6+Sy2Dwh13Rl2keYW5HPWcqPcoCHt3Nlma\nyJajcM59D/heVK8vvWX9ltu474tPUD12lMIdd8auVdBNUe9R0K3uMLk25lwyuuJLpZIrl8tRF0NE\nAqAxg+4xs8POudJCz9NCdVdQrVapVCoUi0UKhcLCvxCAU5O1RG70LnKtutEdJtdGYTCParXK3r17\naTabZLNZRkdHQw+EU5M1Xvz6GM1Zj2xfhp2PDCkQRKRrYj3PICqVSoVms4lzjmazSaVSCf01T56Y\npjnr4Rw0mx4nT2gGalLUakeoVL5JrXYk6qKILJlaBvMoFotks9lLLYNisRj6a27YMkC2L0Oz6ZHN\nZtiwRU3oJKjVjnBk7AE8r04mk2N46Bny+eGoiyVyzRQG8ygUCoyOjnZ1zGDd5jw7HxnSmEHCTE8f\nxPPqgIfnNZiePqgwkERSGFxBoVDo2sDxnHWb8wqBhBkY2Eomk8PzGmQy/QwMaIktSSaFgUgH8vlh\nhoeeYXr6IAMDW9UqkMRSGIh0KJ8fVghI4uluIhERURiIiIjCQK7Bqckah39Q4dRkLeqi9Jy478Ug\nyacxgxCkcVkJzZCOThL2YpDkS33LoFy7wJNTpynXLnTl9ebeNA++OMmLXx9LzadozZCOznx7MYgE\nLdUtg3LtAveOT9DwHP0Z44XBWyjlV4T6mvO9aabhE7RmSEcnKXsxSLKlOgz2nz1Pw3M0ATzH/rPn\nQw+DtL5paoZ0dLQXg3RDqsNg++qV9GcM/JbB9tUrQ3/NNL9pBjVDOo1jKmFbv+U2hYCEKtVhUMqv\n4IXBW9h/9jzbV68MvVUwR8tKXJkGokXiKdVhAK1A6FYIRKFcu9D1sOtEWsdURJIu9WGQZlEMkHcq\nrWMqIkmnMEiwKAbIO5XmMRWRJFMYJFgUA+RB6HRMRQPQIsFTGCRYVAPkUdIAtEg4FAaXGT8zTvl0\nmdLaEoNrBqMuzoLSPkB+OQ1Ai4RDYdBm/Mw4D738EPVmnVw2x+4duxMRCL1EA9Ai4VAYtCmfLlNv\n1vHwaHgNyqfLCoOY0QB0tN46cVwzoVNKYdCmtLZELpuj4TXoz/RTWluKukixEpeBW03qi0aSV09N\nWvdvFBQGbQbXDLJ7x279o5mHBm5lvtVTwwiDi2NjXDz0GstH7mb50FDH11P37+IoDC4zuGYwtH8o\ntdqRxG6croFb6cbqqRfHxvjVZz6Lq9exXI6NTz/VcSCo+3dxFAZdUqsd4cjYA3henUwmx/DQM4kK\nBA3cSjdWT7146DVcvQ6eh2s0Wi2EDsNA3b+LozDokunpg3heHfDwvAbT0wcTFQYauBUIf/XU5SN3\nY7kcrtHA+vtZPnJ3x9dU9+/iKAy6ZGBgK5lMDs9rkMn0MzCwNeoiXTMN3ErYlg8NsfHppwIdM4Bw\nu3/TQmHQJfn8MMNDzyR2zECkW5YPDQUWArJ4CoMuyueHFQIiEkuZqAsgIiLRUxiIiIjCQLrr4tgY\n735rFxfHxqIuioi00ZiBdE0YE4pEJBhqGUjXzDehSETioaMwMLP7zOyYmXlmVrrs3GNmNmFmb5jZ\nx9qO32VmR/1zT5qZdVKGXletVtm3bx/VajXqoixobkIR2WxgE4okXmq1I1Qq36RWO9LdF64egn1f\nbX2XJem0m+inwH8EvtV+0MxuB+4H7gDWA6+Y2RbnXBP4JvAQcBD4HnAP8P0Oy9GTqtUqe/fupdls\nks1mGR0dpVAoRF2sKwprQpHEQ2RLrlQPwd6PQ7MO2RyMvgSFkUAuPTN1jpnJGss251m2aVUg14yr\njsLAOXccYJ4P9zuB55xzM8AvzWwCGDGzCrDKOXfA/71vA59AYbAklUqFZrOJc45ms0mlUol1GIAm\nFKVZZEuuVPa1gsA1W98r+wIJg5mpc7y75yhu1sP6Mlz/4J2pDoSwxgw2AO39Fm/6xzb4jy8/Pi8z\ne9jMymZWfuedd0IpaJIVi0Wy2SxmRjabpVgsRl0k6WFzS65AtrtLrhQ/3GoRWLb1vfjhQC47M1nD\nzXrgwM16zEzWArluXC3YMjCzV4B185x63Dn3YvBFep9zbhewC6BUKrkwXyuJCoUCo6OjVCoVisVi\n7FsFkm6RLblSGGl1DVX2tYIgoC6iZZvzWF/mUstgWcrX5VowDJxzH13CdU8C7e9MN/nHTvqPLz8u\nS1QoFBQCEhuRLblSGAksBOYs27SK6x+8s2fGDMLqJnoJuN/MlpnZzcCtwCHn3NvAOTPb5t9F9Ckg\n1NaFiMhSLdu0ilX/oZD6IIDOby39XTN7E/hXwP8ysx8COOeOAc8DrwM/AD7v30kE8AfAHmAC+AUa\nPBYRiZw5l4yu+FKp5MrlctTFCFXQe7+KiJjZYefcgtu7aTmKmNBSDSISJS1HERNaquHKxs+Ms+fo\nHsbPjEddFJHUUssgJsLY+zUNxs+M89DLD1Fv1sllc+zesVvbF4qEQGEQE1qqYX7l02XqzToeHg2v\nQfl0WWEgEgKFQYykZamGarUa2ES40toSuWyOhtegP9NPae2C42AisgQKAwlU0IvnDa4ZZPeO3ZRP\nlymtLalVIBIShYEEKozF8wbXDCoEREKmu4l6zMzUOc69WmVm6lwo19fieSLJpJZBD+nGkrxaPE8k\nmRQGPWS+JXnDWHNFi+eJJI+6iXrI3JK8GD2xJK+ILJ5aBj2k15bkFZHFUxj0mGWbVikEROQD1E0k\nIiIKAxERURiIiAgKAxERQWEgi1CtVtm3bx/VajXqoohISHQ3kVxV0AvPiUSpXLvA/rPn2b56JaX8\niqiLEysKA7mqMBaeE4lCuXaBe8cnaHiO/ozxwuAtCoQ26iaSq9LCc7IUpyZrHP5BhVOTtaiLcsn+\ns+dpeI4m0PAc+8+ej7pIsaKWgVyVFp6Ta3VqssaLXx+jOeuR7cuw85Eh1sVg6ZPtq1fSnzHwWwbb\nV6+MukixojCQBWnhObkWJ09M05z1cA6aTY+TJ6ZjEQal/ApeGLxFYwZXoDAQkUBt2DJAti9Ds+mR\nzWbYsGUg0OvPTJ1b8vpapfwKhcAVKAxEJFDrNufZ+cgQJ09Ms2HLQKCtgm7sydGrFAYi8msOT01z\nYPI9tm2+jrs2Le1T/brN+VC6hrq1J0cvUhiIyCWHp6b55J4D1Gc9cn0Znn1w25IDIQxze3LMtQy0\nJ0dwFAYicsmByfeoz3p4DhqzHgcm34tXGGhPjtAoDETkkm2bryPXl6Ex69Hfl2Hb5uuiLtIHaE+O\ncCgMROSSuzYN8OyD2zoeM5DkURiIyK+5a9OAQqAHaTkKERFRGIiIiMJARERQGIiICAoDkdR468Rx\nDn73ed46cTzqokgC6W4ikRR468Rx/vZLj9OcnSXb18d9X3yC9Vtui7pYkiBqGYikQPXYUZqzszjP\nozk7S/XY0aiLJAnTURiY2Z+b2c/M7Cdm9l0zW9127jEzmzCzN8zsY23H7zKzo/65J83MOimDiEDh\njjvJ9vVhmQzZvj4Kd9wZdZEkYcw5t/RfNtsB/G/n3KyZfQXAOfcFM7sd+A4wAqwHXgG2OOeaZnYI\n+CPgIPA94Enn3PcXeq1SqeTK5fKSyyrSibdOHKd67CiFO+6MbfdLt8p4cWyMi4deY/nI3SwfGgrt\ndSQYZnbYOVda6HkdjRk4515u+/EAcK//eCfwnHNuBvilmU0AI2ZWAVY55w74hfw28AlgwTAQiUpS\n+uPXb7kt9HJdHBvjV5/5LK5ex3I5Nj79lAIhJYIcM/gs77+pbwCqbefe9I9t8B9fflwkttQf/76L\nh17D1evgebhGg4uHXou6SBKQBVsGZvYKsG6eU4875170n/M4MAs8G2ThzOxh4GGAjRs3BnlpkUWb\n64+faxn0cn/88pG7sVwO12hg/f0sH7l7SdepVqtUKhWKxaL2146JBcPAOffRq503s08DvwN8xL0/\nAHESaP8L3+QfO+k/vvz4lV57F7ALWmMGC5VVJAzrt9zGfV98IvZjBt2wfGiIjU8/1dGYQbVaZe/e\nvTSbTbLZLKOjowqEGOhozMDM7gH+FPh3zrmLbadeAv7GzL5GawD5VuCQP4B8zsy20RpA/hTw3zsp\ng0g3dKM/PimWDw11NE5QqVRoNps452g2m1QqFYVBDHQ66ewvgGXAj/w7RA845/6zc+6YmT0PvE6r\n++jzzrmm/zt/APw18C9ojTFo8FikhxSLRbLZ7KWWQbFYjLpIQoe3lnaTbi0VSQ+NGXRPV24tFRFZ\nikKhoBCIGS1HISIiCgMREVEYiIgICgMREUFhICIiKAxERASFgYiIoDAQ6Rm12hEqlW9Sqx2JuigS\nQ5p0JtIDarUjHBl7AM+rk8nkGB56hnx+OOpiSYyoZSDSA6anD+J5dcDD8xpMTx/s6HrVapV9+/ZR\nrVYXfrIkgloGIj1gYGArmUwOz2uQyfQzMLB1ydfSEtTppDAQ6QH5/DDDQ88wPX2QgYGtHXURaQnq\ndFIYiPSIfH44kHECLUGdTgoDEbkmhUKB0dFRLUGdMgoDEblmWoI6fXQ3kYiIKAxERERhICIiKAxE\nRASFgYiIoDAQERHAnHNRl2FRzOwdYCrEl7geeDfE68dFL9SzF+oIqmfahFXPTc65GxZ6UmLCIGxm\nVnbOlaIuR9h6oZ69UEdQPdMm6nqqm0hERBQGIiKiMGi3K+oCdEkv1LMX6giqZ9pEWk+NGYiIiFoG\nIiLSg2FgZl8ys5+Y2biZvWxm69vOPWZmE2b2hpl9rO34XWZ21D/3pJlZNKVfPDP7czP7mV/X75rZ\n6rZzaarnfWZ2zMw8Mytddi419bycmd3j12vCzB6NujxLZWZPmdkZM/tp27EPmdmPzOzn/veBtnPz\n/k3jzswKZvaqmb3u/3v9Y/94fOrqnOupL2BV2+M/Av7Sf3w78P+AZcDNwC+ArH/uELANMOD7wG9F\nXY9F1HMH0Oc//grwlZTW8zbgXwL/AJTajqeqnpfVOevXZzOQ8+t5e9TlWmJd/i0wDPy07dh/Ax71\nHz+6mH+7cf8CbgSG/ce/AZzw6xObuvZcy8A5d67txxXA3KDJTuA559yMc+6XwAQwYmY30gqQA671\nV/o28ImuFnoJnHMvO+dm/R8PADf5j9NWz+POuTfmOZWqel5mBJhwzk065+rAc7TqmzjOuX8E/umy\nwzuBvf7jvbz/95n3b9qVgnbIOfe2c+6I//ifgePABmJU154LAwAze8LMqsAngf/qH94AVNue9qZ/\nbIP/+PLjSfJZWp+AId31bJfmel6pbmmx1jn3tv/4FLDWf5yKeptZERgCDhKjuqZypzMzewVYN8+p\nx51zLzrnHgceN7PHgD8E/qyrBQzIQvX0n/M4MAs8282yBWkx9ZR0cs45M0vNLY9mthL4O+BPnHPn\n2oeroq5rKsPAOffRRT71WeB7tMLgJNC+j99N/rGTvN/F0n48cgvV08w+DfwO8BG/SwRSWM8rSFw9\nr8GV6pYWp83sRufc23633hn/eKLrbWb9tILgWefc3/uHY1PXnusmMrNb237cCfzMf/wScL+ZLTOz\nm4FbgUN+E+6cmW3z7zr5FBD7T6Nmdg/wp8DHnXMX206lqp5XkeZ6vgbcamY3m1kOuJ9WfdPiJWDU\nfzzK+3+fef+mEZTvmvn/1v4KOO6c+1rbqfjUNepR9m5/0UrmnwI/Af4nsKHt3OO0Ru3foO0OE6Dk\n/84vgL/An6wX5y9aA05VYNz/+suU1vN3afWnzgCngR+msZ7z1Pu3ad2R8gta3WWRl2mJ9fgO8DbQ\n8P+OnwOuA34M/Bx4BfjQQn/TuH8B/4bWzSo/afs/+dtxqqtmIIuISO91E4mIyAcpDERERGEgIiIK\nAxERQWEgIiIoDEREBIWBiIigMBAREeD/A+c5uQGZ3gdxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7577bca7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for label in set(val_labels):\n",
    "    idxs = [i for i,el in enumerate(val_labels) if el == label]\n",
    "    plt.plot(S_table_batch_tsne[idxs, 0], S_table_batch_tsne[idxs, 1], '.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
